{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f400486b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "# Modified from the original `predictor_example.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a8f539",
   "metadata": {},
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a955fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install git+https://github.com/facebookresearch/segment-anything.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ff31cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget, os\n",
    "\n",
    "root_dir = os.getenv('DATA_ROOT')\n",
    "sam_checkpoint = root_dir + \"/sam_vit_h_4b8939.pth\"\n",
    "if not os.path.exists(sam_checkpoint):\n",
    "    wget.download('https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth', sam_checkpoint)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a1ae39ff",
   "metadata": {},
   "source": [
    "# Object masks from prompts with SAM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b4a4b25c",
   "metadata": {},
   "source": [
    "The Segment Anything Model (SAM) predicts object masks given prompts that indicate the desired object. The model first converts the image into an image embedding that allows high quality masks to be efficiently produced from a prompt. \n",
    "\n",
    "The `SamPredictor` class provides an easy interface to the model for prompting the model. It allows the user to first set an image using the `set_image` method, which calculates the necessary image embeddings. Then, prompts can be provided via the `predict` method to efficiently predict masks from those prompts. The model can take as input both point and box prompts, as well as masks from the previous iteration of prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69b28288",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from monai.transforms import EnsureType, LoadImage, SaveImage, ScaleIntensityRangePercentiles\n",
    "from monai.utils import set_determinism\n",
    "from scipy.ndimage import center_of_mass\n",
    "from scipy.ndimage import label as scipy_label\n",
    "from segment_anything import SamPredictor, sam_model_registry\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "23842fb2",
   "metadata": {},
   "source": [
    "## Generate pseudo annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff3fee8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first generate 3D nii mask from `COSMOS` xml files\n",
    "set_determinism(42) # ensure split\n",
    "dataset = 'COSMOS' # 'COSMOS' or 'careII'\n",
    "device = 'cuda:0'\n",
    "root_dir = Path(os.getenv(\"DATA_ROOT\"))\n",
    "\n",
    "sam_checkpoint = root_dir / \"sam_vit_h_4b8939.pth\"\n",
    "sam = sam_model_registry[\"vit_h\"](checkpoint=sam_checkpoint)\n",
    "sam.to(device=device)\n",
    "predictor = SamPredictor(sam)\n",
    "\n",
    "if dataset == 'COSMOS':\n",
    "    data_dir = root_dir / 'COSMOS'/ 'train_data'\n",
    "    all_cases = [f for f in os.listdir(data_dir) if f.isdigit()]\n",
    "else:\n",
    "    data_dir = root_dir / 'careII'/ 'train_data'\n",
    "    all_cases = [folder for folder in os.listdir(data_dir) if folder.startswith(\"0_P\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6a8214",
   "metadata": {},
   "source": [
    "## Define centerline extraction functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cee7e046",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shortest_path(mask: np.ndarray) -> list[np.ndarray]:\n",
    "    mask = EnsureType(\"numpy\", dtype=np.float32)(mask)\n",
    "    C, H, W, D = mask.shape\n",
    "    indice = np.unique(np.where(mask > 0)[-1])  # Get labeled slices idx from 2 channels\n",
    "    center_matrix = np.zeros((len(indice), 3, 3))\n",
    "    component_matrix = np.zeros((len(indice), 3, H, W))\n",
    "    for d, slice_idx in enumerate(indice):\n",
    "        lumen_mask_slice = mask[0, ..., slice_idx]\n",
    "        lumen_mask_slice, num_components = scipy_label(lumen_mask_slice)\n",
    "        for i in range(3):  # top 3 masks\n",
    "            component = lumen_mask_slice == i + 1\n",
    "            if np.any(component):\n",
    "                center = center_of_mass(component)\n",
    "                center_matrix[d, i] = np.array([center[0], center[1], slice_idx])\n",
    "            else:\n",
    "                center_matrix[d, i] = np.array([0, 0, 0])\n",
    "            component_matrix[d, i] = np.float32(component)\n",
    "\n",
    "    center_matrix = np.flip(center_matrix, 0)  # from top of brain to neck\n",
    "    component_matrix = np.flip(component_matrix, 0)\n",
    "    voxel_center = np.mean(center_matrix[:, 0], axis=0)  # 3,\n",
    "    shortest_centers = [np.array([voxel_center[0], voxel_center[1], center_matrix[0][0][-1]])]\n",
    "    shortest_components = [np.zeros_like(component_matrix[0][0])]\n",
    "    for i in range(len(center_matrix)):\n",
    "        current_center = shortest_centers[-1]\n",
    "        start = i\n",
    "        end = min(i + 50, len(center_matrix))\n",
    "        centers_next_50 = center_matrix[start:end]\n",
    "        compos_next_50 = component_matrix[start:end]\n",
    "        distances = np.sum((centers_next_50 - current_center[None, None, ...]) ** 2, axis=-1)  # 50, 3\n",
    "        closest_idx = np.array(np.where(distances == distances.min())).T[0]\n",
    "        shortest_centers.append(centers_next_50[closest_idx[0], closest_idx[1]])\n",
    "        shortest_components.append(compos_next_50[closest_idx[0], closest_idx[1]])\n",
    "\n",
    "    shortest_mask = np.zeros_like(mask)\n",
    "    for center, comp in zip(shortest_centers, shortest_components):\n",
    "        shortest_mask[0, ..., int(center[-1])] += comp\n",
    "    shortest_centers = np.flip(np.array(shortest_centers), 0)\n",
    "\n",
    "    return shortest_centers, shortest_mask\n",
    "\n",
    "\n",
    "def fix_missing_centers(centers: np.ndarray) -> list[np.ndarray]:\n",
    "    r\"\"\"Use interpolation to predict the missing slice center.\"\"\"\n",
    "    idx = 0\n",
    "    new_centers = []\n",
    "    for idx in range(len(centers) - 1):\n",
    "        new_centers.append(centers[idx].astype(np.int32))\n",
    "        ideal_next = int(centers[idx][-1] + 1)\n",
    "        real_next = int(centers[idx + 1][-1])\n",
    "        if real_next != ideal_next:\n",
    "            step = (centers[idx + 1] - centers[idx]) / (real_next - ideal_next)\n",
    "            for i in range(ideal_next, real_next):\n",
    "                new_center = centers[idx] + (i - ideal_next + 1) * step\n",
    "                new_center[-1] = i\n",
    "                new_centers.append(np.int32(new_center))\n",
    "    return new_centers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2626e4d",
   "metadata": {},
   "source": [
    "## Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b089b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = os.path.join(root_dir, dataset, 'preprocessed', 'mri_nii_raw')\n",
    "output_dir = os.path.join(root_dir, dataset, 'preprocessed', 'mri_nii_raw')\n",
    "image_loader = LoadImage(image_only=True, ensure_channel_first=True)\n",
    "image_saver = SaveImage(output_dir=output_dir, output_postfix='', separate_folder=False, print_log=True)\n",
    "for case in all_cases:\n",
    "    sam_save_dir = os.path.join(output_dir, f'{case}_sam.nii.gz')\n",
    "    if os.path.exists(sam_save_dir): # skip existed\n",
    "        continue\n",
    "    image = image_loader(os.path.join(input_dir, f'{case}_image.nii.gz'))\n",
    "    sam_mask = torch.zeros_like(image)\n",
    "    half_width = image.shape[1] // 2\n",
    "    for i in range(2): # we do sam side by side to avoid one side failure pred ruins entire slice\n",
    "        side_mask = image_loader(os.path.join(input_dir, f'{case}_mask.nii.gz'))\n",
    "        side_mask[:, half_width * i: half_width * (i + 1)] = 0 # remove half of the side\n",
    "        side_mask = np.asarray(side_mask == 1, dtype=np.float32)\n",
    "        labeled_slices = np.unique(np.where(side_mask > 0)[-1])\n",
    "        if len(labeled_slices) <= 3:\n",
    "            print(case, i, 'less than 3 annotated')\n",
    "        else:\n",
    "            print(case, i, 'processing')\n",
    "            image = ScaleIntensityRangePercentiles(0, 98, 0, 1, clip=True)(image)\n",
    "            center_line, mask = shortest_path(side_mask) # find center line in 3D sparse annotations\n",
    "            center_line = fix_missing_centers(center_line) # interpolate centerline\n",
    "            for center in tqdm(center_line):\n",
    "                center = np.int32(center)\n",
    "                if center[-1] not in labeled_slices:\n",
    "                    image_slice = image[0, :, :, center[-1]]\n",
    "                    image_slice = np.asarray(image_slice*255, dtype=np.uint8)\n",
    "                    image_slice = cv2.cvtColor(image_slice, cv2.COLOR_GRAY2RGB)                    \n",
    "                    predictor.set_image(image_slice)\n",
    "\n",
    "                    input_point = np.array([[center[1], center[0]]]).astype(int)\n",
    "                    input_label = np.array([1])\n",
    "                    masks_pred, scores, logits = predictor.predict(\n",
    "                        point_coords=input_point,\n",
    "                        point_labels=input_label,\n",
    "                        multimask_output=True,\n",
    "                    )\n",
    "                    mask_pred = (masks_pred[0] > 0).astype(int)\n",
    "                    mask_pred[half_width * i: half_width * (i + 1)] = 0 # ensure\n",
    "                    side_mask[0, :, :, center[-1]] += mask_pred\n",
    "        sam_mask += torch.as_tensor(side_mask)\n",
    "    sam_mask[sam_mask > 0] = 1\n",
    "    sam_mask.meta['filename_or_obj'] = sam_save_dir\n",
    "    image_saver(sam_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89787cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
